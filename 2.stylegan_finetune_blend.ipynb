{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16ac6c-de91-49db-8b20-66a1e8b84d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan3.git\n",
    "!pip install --upgrade psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8114721e-b84e-4c9d-9e0f-c61c4695809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘stylegan3-t-ffhqu-256x256.pkl’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# based on https://github.com/Sxela/stylegan3_blending/blob/main/stylegan3_blending_public.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.abspath(\"\"), \"stylegan3\"))\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "\n",
    "init_model = \"stylegan3-t-ffhqu-256x256.pkl\"\n",
    "trainData = \"arcaneFilteredData.zip\"\n",
    "finetuned_path = \"model_stylegan3_finetuned\"\n",
    "createdDataPath = \"stylegan3_data_arcane/\"\n",
    "createdDataPhotoPath = createdDataPath + \"photo\"\n",
    "createdDataAnimePath = createdDataPath + \"anime\"\n",
    "\n",
    "for i in [finetuned_path, createdDataPhotoPath, createdDataAnimePath]:\n",
    "    os.makedirs(i, exist_ok=True)\n",
    "\n",
    "\n",
    "# Download pretrained checkpoint\n",
    "!wget -nc https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/{init_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35f730-3fce-45e4-8770-cc8c42463bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training options:\n",
      "{\n",
      "  \"G_kwargs\": {\n",
      "    \"class_name\": \"training.networks_stylegan3.Generator\",\n",
      "    \"z_dim\": 512,\n",
      "    \"w_dim\": 512,\n",
      "    \"mapping_kwargs\": {\n",
      "      \"num_layers\": 2\n",
      "    },\n",
      "    \"channel_base\": 16384,\n",
      "    \"channel_max\": 512,\n",
      "    \"magnitude_ema_beta\": 0.9988915792636801\n",
      "  },\n",
      "  \"D_kwargs\": {\n",
      "    \"class_name\": \"training.networks_stylegan2.Discriminator\",\n",
      "    \"block_kwargs\": {\n",
      "      \"freeze_layers\": 10\n",
      "    },\n",
      "    \"mapping_kwargs\": {},\n",
      "    \"epilogue_kwargs\": {\n",
      "      \"mbstd_group_size\": 4\n",
      "    },\n",
      "    \"channel_base\": 16384,\n",
      "    \"channel_max\": 512\n",
      "  },\n",
      "  \"G_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.0025\n",
      "  },\n",
      "  \"D_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.002\n",
      "  },\n",
      "  \"loss_kwargs\": {\n",
      "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
      "    \"r1_gamma\": 2.0,\n",
      "    \"blur_init_sigma\": 0\n",
      "  },\n",
      "  \"data_loader_kwargs\": {\n",
      "    \"pin_memory\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"num_workers\": 3\n",
      "  },\n",
      "  \"training_set_kwargs\": {\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
      "    \"path\": \"../arcaneFilteredData1.zip\",\n",
      "    \"use_labels\": false,\n",
      "    \"max_size\": 11505,\n",
      "    \"xflip\": true,\n",
      "    \"resolution\": 256,\n",
      "    \"random_seed\": 0\n",
      "  },\n",
      "  \"num_gpus\": 1,\n",
      "  \"batch_size\": 32,\n",
      "  \"batch_gpu\": 16,\n",
      "  \"metrics\": [\n",
      "    \"fid50k_full\"\n",
      "  ],\n",
      "  \"total_kimg\": 25000,\n",
      "  \"kimg_per_tick\": 4,\n",
      "  \"image_snapshot_ticks\": 10,\n",
      "  \"network_snapshot_ticks\": 10,\n",
      "  \"random_seed\": 0,\n",
      "  \"ema_kimg\": 10.0,\n",
      "  \"augment_kwargs\": {\n",
      "    \"class_name\": \"training.augment.AugmentPipe\",\n",
      "    \"xflip\": 1,\n",
      "    \"rotate90\": 1,\n",
      "    \"xint\": 1,\n",
      "    \"scale\": 1,\n",
      "    \"rotate\": 1,\n",
      "    \"aniso\": 1,\n",
      "    \"xfrac\": 1,\n",
      "    \"brightness\": 1,\n",
      "    \"contrast\": 1,\n",
      "    \"lumaflip\": 1,\n",
      "    \"hue\": 1,\n",
      "    \"saturation\": 1\n",
      "  },\n",
      "  \"ada_target\": 0.6,\n",
      "  \"resume_pkl\": \"../stylegan3-t-ffhqu-256x256.pkl\",\n",
      "  \"ada_kimg\": 100,\n",
      "  \"ema_rampup\": null,\n",
      "  \"run_dir\": \"../model_stylegan3_finetuned/00006-stylegan3-t-arcaneFilteredData1-gpus1-batch32-gamma2\"\n",
      "}\n",
      "\n",
      "Output directory:    ../model_stylegan3_finetuned/00006-stylegan3-t-arcaneFilteredData1-gpus1-batch32-gamma2\n",
      "Number of GPUs:      1\n",
      "Batch size:          32 images\n",
      "Training duration:   25000 kimg\n",
      "Dataset path:        ../arcaneFilteredData1.zip\n",
      "Dataset size:        11505 images\n",
      "Dataset resolution:  256\n",
      "Dataset labels:      False\n",
      "Dataset x-flips:     True\n",
      "\n",
      "Creating output directory...\n",
      "Launching processes...\n",
      "Loading training set...\n",
      "\n",
      "Num images:  23010\n",
      "Image shape: [3, 256, 256]\n",
      "Label shape: [0]\n",
      "\n",
      "Constructing networks...\n",
      "Resuming from \"../stylegan3-t-ffhqu-256x256.pkl\"\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n",
      "\n",
      "Generator                     Parameters  Buffers  Output shape         Datatype\n",
      "---                           ---         ---      ---                  ---     \n",
      "mapping.fc0                   262656      -        [16, 512]            float32 \n",
      "mapping.fc1                   262656      -        [16, 512]            float32 \n",
      "mapping                       -           512      [16, 16, 512]        float32 \n",
      "synthesis.input.affine        2052        -        [16, 4]              float32 \n",
      "synthesis.input               262144      1545     [16, 512, 36, 36]    float32 \n",
      "synthesis.L0_36_512.affine    262656      -        [16, 512]            float32 \n",
      "synthesis.L0_36_512           2359808     25       [16, 512, 36, 36]    float32 \n",
      "synthesis.L1_36_512.affine    262656      -        [16, 512]            float32 \n",
      "synthesis.L1_36_512           2359808     25       [16, 512, 36, 36]    float32 \n",
      "synthesis.L2_36_512.affine    262656      -        [16, 512]            float32 \n",
      "synthesis.L2_36_512           2359808     25       [16, 512, 36, 36]    float32 \n",
      "synthesis.L3_52_512.affine    262656      -        [16, 512]            float32 \n",
      "synthesis.L3_52_512           2359808     37       [16, 512, 52, 52]    float16 \n",
      "synthesis.L4_52_512.affine    262656      -        [16, 512]            float32 \n",
      "synthesis.L4_52_512           2359808     25       [16, 512, 52, 52]    float16 \n",
      "synthesis.L5_84_512.affine    262656      -        [16, 512]            float32 \n",
      "synthesis.L5_84_512           2359808     37       [16, 512, 84, 84]    float16 \n",
      "synthesis.L6_84_512.affine    262656      -        [16, 512]            float32 \n",
      "synthesis.L6_84_512           2359808     25       [16, 512, 84, 84]    float16 \n",
      "synthesis.L7_148_362.affine   262656      -        [16, 512]            float32 \n",
      "synthesis.L7_148_362          1668458     37       [16, 362, 148, 148]  float16 \n",
      "synthesis.L8_148_256.affine   185706      -        [16, 362]            float32 \n",
      "synthesis.L8_148_256          834304      25       [16, 256, 148, 148]  float16 \n",
      "synthesis.L9_148_181.affine   131328      -        [16, 256]            float32 \n",
      "synthesis.L9_148_181          417205      25       [16, 181, 148, 148]  float16 \n",
      "synthesis.L10_276_128.affine  92853       -        [16, 181]            float32 \n",
      "synthesis.L10_276_128         208640      37       [16, 128, 276, 276]  float16 \n",
      "synthesis.L11_276_91.affine   65664       -        [16, 128]            float32 \n",
      "synthesis.L11_276_91          104923      25       [16, 91, 276, 276]   float16 \n",
      "synthesis.L12_276_64.affine   46683       -        [16, 91]             float32 \n",
      "synthesis.L12_276_64          52480       25       [16, 64, 276, 276]   float16 \n",
      "synthesis.L13_256_64.affine   32832       -        [16, 64]             float32 \n",
      "synthesis.L13_256_64          36928       25       [16, 64, 256, 256]   float16 \n",
      "synthesis.L14_256_3.affine    32832       -        [16, 64]             float32 \n",
      "synthesis.L14_256_3           195         1        [16, 3, 256, 256]    float16 \n",
      "synthesis                     -           -        [16, 3, 256, 256]    float32 \n",
      "---                           ---         ---      ---                  ---     \n",
      "Total                         23320443    2456     -                    -       \n",
      "\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "\n",
      "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
      "---            ---         ---      ---                  ---     \n",
      "b256.fromrgb   -           272      [16, 64, 256, 256]   float16 \n",
      "b256.skip      -           8208     [16, 128, 128, 128]  float16 \n",
      "b256.conv0     -           36944    [16, 64, 256, 256]   float16 \n",
      "b256.conv1     -           73872    [16, 128, 128, 128]  float16 \n",
      "b256           -           16       [16, 128, 128, 128]  float16 \n",
      "b128.skip      -           32784    [16, 256, 64, 64]    float16 \n",
      "b128.conv0     -           147600   [16, 128, 128, 128]  float16 \n",
      "b128.conv1     -           295184   [16, 256, 64, 64]    float16 \n",
      "b128           -           16       [16, 256, 64, 64]    float16 \n",
      "b64.skip       -           131088   [16, 512, 32, 32]    float16 \n",
      "b64.conv0      -           590096   [16, 256, 64, 64]    float16 \n",
      "b64.conv1      -           1180176  [16, 512, 32, 32]    float16 \n",
      "b64            -           16       [16, 512, 32, 32]    float16 \n",
      "b32.skip       262144      16       [16, 512, 16, 16]    float16 \n",
      "b32.conv0      2359808     16       [16, 512, 32, 32]    float16 \n",
      "b32.conv1      2359808     16       [16, 512, 16, 16]    float16 \n",
      "b32            -           16       [16, 512, 16, 16]    float16 \n",
      "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
      "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
      "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
      "b16            -           16       [16, 512, 8, 8]      float32 \n",
      "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
      "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
      "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
      "b8             -           16       [16, 512, 4, 4]      float32 \n",
      "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
      "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
      "b4.fc          4194816     -        [16, 512]            float32 \n",
      "b4.out         513         -        [16, 1]              float32 \n",
      "---            ---         ---      ---                  ---     \n",
      "Total          21505025    2496480  -                    -       \n",
      "\n",
      "Setting up augmentation...\n",
      "Distributing across 1 GPUs...\n",
      "Setting up training phases...\n",
      "Exporting sample images...\n",
      "Initializing logs...\n",
      "Training for 25000 kimg...\n",
      "\n",
      "tick 0     kimg 0.0      time 1m 16s       sec/tick 14.2    sec/kimg 443.56  maintenance 61.8   cpumem 5.87   gpumem 9.10   reserved 9.26   augment 0.000\n",
      "Evaluating metrics...\n",
      "{\"results\": {\"fid50k_full\": 124.95765432763801}, \"metric\": \"fid50k_full\", \"total_time\": 1405.2369103431702, \"total_time_str\": \"23m 25s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1641058648.5700772}\n",
      "tick 1     kimg 4.0      time 32m 49s      sec/tick 460.2   sec/kimg 115.04  maintenance 1432.9 cpumem 6.19   gpumem 7.59   reserved 9.10   augment 0.038\n",
      "tick 2     kimg 8.0      time 40m 30s      sec/tick 460.8   sec/kimg 115.20  maintenance 0.1    cpumem 6.19   gpumem 6.72   reserved 9.10   augment 0.078\n",
      "tick 3     kimg 12.0     time 48m 11s      sec/tick 461.0   sec/kimg 115.25  maintenance 0.1    cpumem 6.19   gpumem 6.73   reserved 9.10   augment 0.116\n",
      "tick 4     kimg 16.0     time 55m 52s      sec/tick 461.2   sec/kimg 115.30  maintenance 0.0    cpumem 6.19   gpumem 6.77   reserved 9.10   augment 0.156\n",
      "tick 5     kimg 20.0     time 1h 03m 33s   sec/tick 461.2   sec/kimg 115.30  maintenance 0.1    cpumem 6.19   gpumem 6.73   reserved 9.10   augment 0.191\n",
      "tick 6     kimg 24.0     time 1h 11m 14s   sec/tick 460.8   sec/kimg 115.20  maintenance 0.1    cpumem 6.19   gpumem 6.77   reserved 9.10   augment 0.207\n",
      "tick 7     kimg 28.0     time 1h 18m 56s   sec/tick 461.4   sec/kimg 115.34  maintenance 0.1    cpumem 6.19   gpumem 6.75   reserved 9.10   augment 0.223\n",
      "tick 8     kimg 32.0     time 1h 26m 37s   sec/tick 461.4   sec/kimg 115.34  maintenance 0.0    cpumem 6.19   gpumem 6.78   reserved 9.10   augment 0.229\n",
      "tick 9     kimg 36.0     time 1h 34m 19s   sec/tick 461.4   sec/kimg 115.34  maintenance 0.1    cpumem 6.19   gpumem 6.78   reserved 9.10   augment 0.233\n",
      "tick 10    kimg 40.0     time 1h 42m 00s   sec/tick 461.4   sec/kimg 115.34  maintenance 0.1    cpumem 6.19   gpumem 6.78   reserved 9.10   augment 0.252\n",
      "Evaluating metrics...\n",
      "{\"results\": {\"fid50k_full\": 19.63360383290616}, \"metric\": \"fid50k_full\", \"total_time\": 1370.027738571167, \"total_time_str\": \"22m 50s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000040.pkl\", \"timestamp\": 1641064658.6509068}\n",
      "tick 11    kimg 44.0     time 2h 13m 00s   sec/tick 461.2   sec/kimg 115.31  maintenance 1399.0 cpumem 6.17   gpumem 6.79   reserved 9.10   augment 0.251\n",
      "tick 12    kimg 48.0     time 2h 20m 42s   sec/tick 461.7   sec/kimg 115.42  maintenance 0.0    cpumem 6.17   gpumem 6.81   reserved 9.10   augment 0.255\n"
     ]
    }
   ],
   "source": [
    "#train stylegan\n",
    "!cd stylegan3 && python train.py --outdir=\"../$finetuned_path\" --data=\"../$trainData\" \\\n",
    "    --cfg=stylegan3-t --gpus=1 --batch=32 --gamma=2 --batch-gpu=16 --snap=10 \\\n",
    "    --mirror=1 --freezed=10 --cbase=16384 --resume=\"../$init_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7968b41b-1286-43ab-9231-36e27165501c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_stylegan3_finetuned/00006-stylegan3-t-arcaneFilteredData1-gpus1-batch32-gamma2/network-snapshot-000160.pkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get recent finetuned model weight path\n",
    "def getRecentPkl(path):\n",
    "    pklList=glob(finetuned_path+\"/**/*.pkl\", recursive=True)\n",
    "    pklList.sort()\n",
    "    return pklList[-1]\n",
    "\n",
    "finetuned_model=getRecentPkl(finetuned_path)\n",
    "finetuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39307ec-db98-4c57-adee-7e78cb2c312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def get_model(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        _G = pickle.load(f)['G_ema'].cuda()\n",
    "    return _G\n",
    "\n",
    "G_raw = get_model(init_model)\n",
    "G_tuned = get_model(finetuned_model)\n",
    "G_blend = copy.deepcopy(G_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bfd9249-c28c-473e-8794-025fdb9d8925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6187a7afa04e4e98d574a2d98ac4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#blend model weight and create data pair\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def doBlend(blendRatio): \n",
    "    # Not blending affine layers gives us colors closer to the original gen, without affecting the geometry much.\n",
    "    W_new = G_raw.synthesis.state_dict().copy()\n",
    "    W_tuned = G_tuned.synthesis.state_dict()\n",
    "    for key in W_new:\n",
    "        if \"input\" in key or 'affine' in key: continue\n",
    "        l = blendRatio[int(key.split('_')[0][1:])]\n",
    "        W_new[key] = W_tuned[key]*l + W_new[key]*(1-l)\n",
    "    G_blend.synthesis.load_state_dict(W_new)\n",
    "    return G_blend\n",
    "\n",
    "def getImage(model,seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    z = torch.randn(1,model.z_dim).cuda()\n",
    "    w = model.mapping(z, None, truncation_psi=.5, truncation_cutoff=8)\n",
    "    image = model.synthesis(w, noise_mode='const', force_fp32=True).cpu()[0]\n",
    "    image = to_pil_image((image * 0.5 + 0.5).clip(0, 1))\n",
    "    return image\n",
    "\n",
    "\n",
    "blendRatio1 = [0,0,0,0,0,0,0,.2,.5,.7,.9,1,1,1,1]\n",
    "blendRatio2 = [0,0,0,0,0,0,0,.2,.5,.7,.8,.8,.8,.8,.8]\n",
    "blendRatio3 = [0,0,0,0,0,.2,.2,.2,.5,.7,.8,.8,.8,.8,1]\n",
    "blendRatio4 = [0]*7+[0.8]*(15-7\n",
    "G_blend=doBlend(blendRatio3)\n",
    "imageNum=10000\n",
    "\n",
    "\n",
    "\n",
    "for i in trange(imageNum):\n",
    "    im1 = getImage(G_raw,seed=i)\n",
    "    im3 = getImage(G_blend,seed=i)\n",
    "    \n",
    "    im1.save(f'{createdDataPhotoPath}/s{i}_.jpg', quality=100, subsampling=0)\n",
    "    im3.save(f'{createdDataAnimePath}/s{i}_.jpg', quality=100, subsampling=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0a2e1-bf40-4d6e-b108-47df0ef8edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display created image\n",
    "for i in trange(0,10):\n",
    "    im1 = getImage(G_raw,seed=i)\n",
    "    im2 = getImage(G_tuned,seed=i)\n",
    "    im3 = getImage(G_blend,seed=i)\n",
    "\n",
    "    display(im1)\n",
    "    display(im2)\n",
    "    display(im3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
